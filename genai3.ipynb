{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-28T15:33:22.919581Z",
     "iopub.status.busy": "2025-10-28T15:33:22.919257Z",
     "iopub.status.idle": "2025-10-28T15:33:28.643511Z",
     "shell.execute_reply": "2025-10-28T15:33:28.642677Z",
     "shell.execute_reply.started": "2025-10-28T15:33:22.919555Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'project (Python 3.13.5)' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages.\n",
      "\u001b[1;31mOr install 'ipykernel' using the command: 'conda install -n project ipykernel --update-deps --force-reinstall'"
     ]
    }
   ],
   "source": [
    "# !pip install transformers datasets accelerate torch pandas -q\n",
    "\n",
    "import pandas as pd\n",
    "import ast  # For safely evaluating string-lists\n",
    "import torch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import GPT2Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T15:33:45.629358Z",
     "iopub.status.busy": "2025-10-28T15:33:45.628899Z",
     "iopub.status.idle": "2025-10-28T15:34:29.837573Z",
     "shell.execute_reply": "2025-10-28T15:34:29.836668Z",
     "shell.execute_reply.started": "2025-10-28T15:33:45.629336Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading full dataset...\n",
      "Loaded 2231143 total recipes.\n",
      "Found 2231142 complete (non-null) recipes.\n",
      "Using a sample of 100000 recipes for preprocessing.\n"
     ]
    }
   ],
   "source": [
    "# Define the path to your dataset in Kaggle\n",
    "file_path = '/kaggle/input/3a2mext/3A2M_EXTENDED.csv'\n",
    "\n",
    "# Load the full dataset\n",
    "print(\"Loading full dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Loaded {len(df)} total recipes.\")\n",
    "\n",
    "# We only need these three columns\n",
    "df_clean = df[['title', 'NER', 'directions']].dropna().reset_index(drop=True)\n",
    "print(f\"Found {len(df_clean)} complete (non-null) recipes.\")\n",
    "\n",
    "# --- Create a smaller sample for faster development ---\n",
    "# Adjust this number based on your Kaggle instance's performance\n",
    "SAMPLE_SIZE = 100000 \n",
    "df_sample = df_clean.sample(n=min(SAMPLE_SIZE, len(df_clean)), random_state=42)\n",
    "print(f\"Using a sample of {len(df_sample)} recipes for preprocessing.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T15:34:40.299963Z",
     "iopub.status.busy": "2025-10-28T15:34:40.299650Z",
     "iopub.status.idle": "2025-10-28T15:34:45.338474Z",
     "shell.execute_reply": "2025-10-28T15:34:45.337547Z",
     "shell.execute_reply.started": "2025-10-28T15:34:40.299935Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatting data into structured text...\n",
      "Successfully processed 88101 recipes.\n",
      "\n",
      "--- Example of Formatted Text ---\n",
      "TITLE: Sweet Potato Pecan Salad\n",
      "INGREDIENTS: stalks celery, sweet potatoes, red onion, olive oil, hanout, fresh rosemary, Kosher salt, green grapes, pecans, pomegranate seeds, scallions, golden raisins, Sherry vinegar\n",
      "RECIPE:\n",
      "Preheat the oven to 400 degrees F. Place a baking sheet in the oven to heat.\n",
      "Toss the celery, sweet potatoes and red onions with the olive oil, ras el hanout and rosemary in a mixing bowl.\n",
      "Season with salt and pepper.\n",
      "Transfer the mixture to the heated baking sheet and roast until the potatoes are tender and caramelized, about 30 minutes.\n",
      "Let the sweet potato mixture cool to room temperature before proceeding.\n",
      "(Or, if preparing ahead of time, cool the sweet potato mixture in the fridge, removing it 30 minutes before proceeding to allow it to come to room temperature.)\n",
      "Toss the sweet potato mixture in a large bowl with the grapes, pecans, pomegranate seeds, scallions and raisins.\n",
      "Toss the salad with sherry vinegar and olive oil to taste.\n",
      "Add salt and pepper as needed.\n"
     ]
    }
   ],
   "source": [
    "def format_recipe(row):\n",
    "    \"\"\"\n",
    "    Safely parses and formats a single row into our structured text format.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        title = row['title']\n",
    "        \n",
    "        # Safely parse the 'NER' string-list into a Python list\n",
    "        ingredients_list = ast.literal_eval(row['NER'])\n",
    "        # Join the list into a single comma-separated string\n",
    "        ingredients_str = \", \".join(ingredients_list)\n",
    "        \n",
    "        # Safely parse the 'directions' string-list\n",
    "        directions_list = ast.literal_eval(row['directions'])\n",
    "        # Join the steps with a newline character\n",
    "        directions_str = \"\\n\".join(directions_list)\n",
    "        \n",
    "        # Return None if any part is empty after parsing\n",
    "        if not title or not ingredients_str or not directions_str:\n",
    "            return None\n",
    "            \n",
    "        # This is the final format our model will learn\n",
    "        return f\"TITLE: {title}\\nINGREDIENTS: {ingredients_str}\\nRECIPE:\\n{directions_str}\"\n",
    "        \n",
    "    except (ValueError, SyntaxError):\n",
    "        # Handle cases where ast.literal_eval fails (e.g., malformed string)\n",
    "        return None\n",
    "\n",
    "print(\"Formatting data into structured text...\")\n",
    "df_sample['text'] = df_sample.apply(format_recipe, axis=1)\n",
    "\n",
    "# Drop any rows that failed parsing (returned None)\n",
    "df_processed = df_sample.dropna(subset=['text']).reset_index(drop=True)\n",
    "\n",
    "print(f\"Successfully processed {len(df_processed)} recipes.\")\n",
    "print(\"\\n--- Example of Formatted Text ---\")\n",
    "print(df_processed['text'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "pip install --upgrade transformers huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T15:35:14.392265Z",
     "iopub.status.busy": "2025-10-28T15:35:14.391724Z",
     "iopub.status.idle": "2025-10-28T15:37:11.389554Z",
     "shell.execute_reply": "2025-10-28T15:37:11.388869Z",
     "shell.execute_reply.started": "2025-10-28T15:35:14.392237Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45b0e1b091df46c8a9b210496c80b6bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "31fa3898581d4bd69e40b47f328a9573",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb7a9ff7792b41bc9f308669b8460a06",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "493b5592ff0a46618ae82c1b1e3c6b14",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4215ca5ab3a34ba8a6e054daef88b8be",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizing 79290 texts...\n",
      "Tokenization complete.\n",
      "Tokenizing 8811 texts...\n",
      "Tokenization complete.\n",
      "\n",
      "Created 79290 training examples.\n",
      "Created 8811 validation examples.\n",
      "\n",
      "--- Example Dataset Item ---\n",
      "Input IDs shape: torch.Size([512])\n",
      "Labels shape: torch.Size([512])\n",
      "Attention Mask shape: torch.Size([512])\n"
     ]
    }
   ],
   "source": [
    "# Load the pre-trained GPT-2 tokenizer\n",
    "tokenizer = GPT2Tokenizer.from_pretrained('gpt2')\n",
    "\n",
    "# GPT-2 needs a padding token. We'll use the end-of-sequence (eos) token.\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "class RecipeDataset(Dataset):\n",
    "    def __init__(self, texts, tokenizer, max_length=512):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        self.encodings = {\n",
    "            \"input_ids\": [],\n",
    "            \"attention_mask\": []\n",
    "        }\n",
    "        \n",
    "        print(f\"Tokenizing {len(texts)} texts...\")\n",
    "        for text in texts:\n",
    "            # Add the eos_token to signal the end of a complete recipe\n",
    "            tokenized = tokenizer(\n",
    "                text + tokenizer.eos_token, \n",
    "                truncation=True, \n",
    "                max_length=self.max_length, \n",
    "                padding=\"max_length\"\n",
    "            )\n",
    "            self.encodings[\"input_ids\"].append(torch.tensor(tokenized['input_ids']))\n",
    "            self.encodings[\"attention_mask\"].append(torch.tensor(tokenized['attention_mask']))\n",
    "        print(\"Tokenization complete.\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.encodings['input_ids'])\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # The Trainer expects a dictionary\n",
    "        item = {\n",
    "            'input_ids': self.encodings['input_ids'][idx],\n",
    "            'attention_mask': self.encodings['attention_mask'][idx],\n",
    "        }\n",
    "        \n",
    "        # For Causal LM, the 'labels' are the same as the 'input_ids'\n",
    "        # The model learns to predict the next token at each step.\n",
    "        item['labels'] = item['input_ids'].clone()\n",
    "        \n",
    "        return item\n",
    "\n",
    "# --- Create the training and validation datasets ---\n",
    "\n",
    "# 1. Split our processed DataFrame text column\n",
    "train_texts, val_texts = train_test_split(\n",
    "    df_processed['text'], \n",
    "    test_size=0.1, \n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 2. Create the Dataset objects\n",
    "train_dataset = RecipeDataset(train_texts.tolist(), tokenizer)\n",
    "val_dataset = RecipeDataset(val_texts.tolist(), tokenizer)\n",
    "\n",
    "print(f\"\\nCreated {len(train_dataset)} training examples.\")\n",
    "print(f\"Created {len(val_dataset)} validation examples.\")\n",
    "\n",
    "# You can inspect a single item to see the output\n",
    "print(\"\\n--- Example Dataset Item ---\")\n",
    "item = train_dataset[0]\n",
    "print(\"Input IDs shape:\", item['input_ids'].shape)\n",
    "print(\"Labels shape:\", item['labels'].shape)\n",
    "print(\"Attention Mask shape:\", item['attention_mask'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip uninstall -y transformers\n",
    "!pip install --no-cache-dir transformers==4.57.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import transformers\n",
    "print(transformers.__version__)\n",
    "from transformers import TrainingArguments\n",
    "print(TrainingArguments)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T15:37:22.750138Z",
     "iopub.status.busy": "2025-10-28T15:37:22.749336Z",
     "iopub.status.idle": "2025-10-28T18:05:16.503711Z",
     "shell.execute_reply": "2025-10-28T18:05:16.502828Z",
     "shell.execute_reply.started": "2025-10-28T15:37:22.750082Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-28 15:37:24.527879: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1761665844.700743      99 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1761665844.750599      99 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Using device: CUDA ---\n",
      "GPU is available\n",
      "Loading pre-trained GPT-2 model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1124b2a96f1646f79c0f01cad28c7578",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "448f00611e7e4fa58452715ffe4eebb7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_99/184480541.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': 50256}.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting up training arguments...\n",
      "39645\n",
      "Starting the fine-tuning process...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`loss_type=None` was set in the config but it is unrecognized. Using the default loss: `ForCausalLMLoss`.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='39645' max='39645' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [39645/39645 2:27:32, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>39645</td>\n",
       "      <td>0.616000</td>\n",
       "      <td>0.622947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['lm_head.weight'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training complete. Saving the final model...\n",
      "Model and tokenizer saved to ./final_recipe_model\n"
     ]
    }
   ],
   "source": [
    "from transformers import GPT2LMHeadModel, Trainer, TrainingArguments\n",
    "import torch\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"--- Using device: {device.upper()} ---\")\n",
    "\n",
    "if device == 'cpu':\n",
    "    print(\"No GPU found\")\n",
    "    use_fp16 = False\n",
    "else:\n",
    "    print(\"GPU is available\")\n",
    "    use_fp16 = True\n",
    "\n",
    "# 2. Load the Pre-trained Model\n",
    "print(\"Loading pre-trained GPT-2 model...\")\n",
    "model = GPT2LMHeadModel.from_pretrained('gpt2')\n",
    "\n",
    "# Resize token embeddings to match the tokenizer (which has the pad token)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# --- Move model to the correct device ---\n",
    "model.to(device)\n",
    "\n",
    "# 3. Define Training Arguments (using older syntax)\n",
    "print(\"Setting up training arguments...\")\n",
    "\n",
    "# Calculate steps per epoch for the older argument style\n",
    "steps_per_epoch = len(train_dataset) // 2  # Using 2 for per_device_train_batch_size\n",
    "if steps_per_epoch == 0:\n",
    "    steps_per_epoch = 1 # Avoid division by zero\n",
    "\n",
    "print(steps_per_epoch)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./recipe_model',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    eval_strategy=\"steps\",          # <- correct for this build\n",
    "    eval_steps=steps_per_epoch,\n",
    "    save_steps=steps_per_epoch,\n",
    "    logging_steps=100,\n",
    "    load_best_model_at_end=True,\n",
    "    fp16=use_fp16,\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 4. Initialize the Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer  \n",
    ")\n",
    "\n",
    "# 5. Start Training\n",
    "print(\"Starting the fine-tuning process...\")\n",
    "trainer.train()\n",
    "\n",
    "# 6. Save the Final Model and Tokenizer\n",
    "print(\"Training complete. Saving the final model...\")\n",
    "final_model_path = './final_recipe_model'\n",
    "trainer.save_model(final_model_path)\n",
    "tokenizer.save_pretrained(final_model_path) \n",
    "\n",
    "print(f\"Model and tokenizer saved to {final_model_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T18:28:03.508262Z",
     "iopub.status.busy": "2025-10-28T18:28:03.507649Z",
     "iopub.status.idle": "2025-10-28T18:28:04.704954Z",
     "shell.execute_reply": "2025-10-28T18:28:04.704134Z",
     "shell.execute_reply.started": "2025-10-28T18:28:03.508238Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer from ./final_recipe_model...\n",
      "Using device: cuda\n",
      "Model loaded. Encoding prompt...\n",
      "Generating recipe...\n",
      "\n",
      "--- GENERATED RECIPE ---\n",
      "Simple Chicken and Rice\n",
      "INGREDIENTS: chicken breasts, flour, salt, black pepper, onion, vegetable oil, garlic, mushrooms, water, rice\n",
      "RECIPE:\n",
      "Heat oil in a deep-fryer or wok.\n",
      "Add chicken, mushrooms and onions; cook and stir until chicken is cooked through.\n",
      "Add remaining ingredients; bring to a boil.\n",
      "Reduce heat; cover and simmer about 20 minutes or until rice is tender.\n",
      "Serve with rice.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "\n",
    "def generate_recipe(model_path, prompt, max_length=300):\n",
    "    \"\"\"\n",
    "    Loads a fine-tuned GPT-2 model and tokenizer from a specified path\n",
    "    and generates text based on a given prompt.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Loading model and tokenizer from {model_path}...\")\n",
    "    \n",
    "    # Check for GPU\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "\n",
    "    # Load the tokenizer and model\n",
    "    try:\n",
    "        tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "        model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading model: {e}\")\n",
    "        print(\"Please ensure the path is correct and all files (config.json, model.safetensors, etc.) are present.\")\n",
    "        return\n",
    "\n",
    "    # Set the pad token to the eos token if it's not already set\n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        model.config.pad_token_id = model.config.eos_token_id\n",
    "\n",
    "    # Move model to the selected device\n",
    "    model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode (important!)\n",
    "\n",
    "    print(\"Model loaded. Encoding prompt...\")\n",
    "    \n",
    "    # Encode the input prompt\n",
    "    input_ids = tokenizer.encode(prompt, return_tensors='pt').to(device)\n",
    "\n",
    "    print(\"Generating recipe...\")\n",
    "\n",
    "    # Generate text\n",
    "    with torch.no_grad():  # Disable gradient calculations for inference\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=max_length,\n",
    "            temperature=0.7,       # Makes the output less random\n",
    "            top_k=50,              # Considers the top 50 most likely words\n",
    "            top_p=0.95,            # Uses nucleus sampling\n",
    "            do_sample=True,        # Enables sampling\n",
    "            num_return_sequences=1,# We just want one recipe\n",
    "            pad_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "\n",
    "    # Decode the generated sequence\n",
    "    generated_text = tokenizer.decode(output_sequences[0], skip_special_tokens=True)\n",
    "    \n",
    "    return generated_text\n",
    "\n",
    "# --- How to use the function ---\n",
    "\n",
    "# 1. Define the path to your saved model\n",
    "model_directory = \"./final_recipe_model\"\n",
    "\n",
    "# 2. Define your starting prompt\n",
    "#    (Try to match the format of your training data)\n",
    "input_prompt = \"Simple Chicken and Rice\" \n",
    "\n",
    "# 3. Generate the recipe\n",
    "recipe_output = generate_recipe(model_directory, input_prompt, max_length=300)\n",
    "\n",
    "# 4. Print the result\n",
    "if recipe_output:\n",
    "    print(\"\\n--- GENERATED RECIPE ---\")\n",
    "    print(recipe_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T18:29:25.325919Z",
     "iopub.status.busy": "2025-10-28T18:29:25.325286Z",
     "iopub.status.idle": "2025-10-28T18:29:50.732231Z",
     "shell.execute_reply": "2025-10-28T18:29:50.731487Z",
     "shell.execute_reply.started": "2025-10-28T18:29:25.325897Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  adding: final_recipe_model/ (stored 0%)\n",
      "  adding: final_recipe_model/vocab.json (deflated 68%)\n",
      "  adding: final_recipe_model/merges.txt (deflated 53%)\n",
      "  adding: final_recipe_model/tokenizer_config.json (deflated 56%)\n",
      "  adding: final_recipe_model/special_tokens_map.json (deflated 74%)\n",
      "  adding: final_recipe_model/config.json (deflated 52%)\n",
      "  adding: final_recipe_model/generation_config.json (deflated 31%)\n",
      "  adding: final_recipe_model/model.safetensors (deflated 7%)\n",
      "  adding: final_recipe_model/training_args.bin (deflated 52%)\n"
     ]
    }
   ],
   "source": [
    "!zip -r my_model.zip ./final_recipe_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T18:33:18.416628Z",
     "iopub.status.busy": "2025-10-28T18:33:18.416292Z",
     "iopub.status.idle": "2025-10-28T18:33:27.151002Z",
     "shell.execute_reply": "2025-10-28T18:33:27.150261Z",
     "shell.execute_reply.started": "2025-10-28T18:33:18.416600Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m47.7/47.7 MB\u001b[0m \u001b[31m35.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "pylibcudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "cudf-cu12 25.2.2 requires pyarrow<20.0.0a0,>=14.0.0; platform_machine == \"x86_64\", but you have pyarrow 22.0.0 which is incompatible.\n",
      "bigframes 2.12.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.12.0 requires rich<14,>=12.4.4, but you have rich 14.1.0 which is incompatible.\n",
      "cudf-polars-cu12 25.6.0 requires pylibcudf-cu12==25.6.*, but you have pylibcudf-cu12 25.2.2 which is incompatible.\n",
      "pandas-gbq 0.29.2 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0m"
     ]
    }
   ],
   "source": [
    "!pip install evaluate rouge_score nltk -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-28T18:35:01.334496Z",
     "iopub.status.busy": "2025-10-28T18:35:01.334159Z",
     "iopub.status.idle": "2025-10-28T18:38:28.983063Z",
     "shell.execute_reply": "2025-10-28T18:38:28.982475Z",
     "shell.execute_reply.started": "2025-10-28T18:35:01.334467Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model and tokenizer...\n",
      "Model loaded and on device: cuda\n",
      "Generating 200 predictions for evaluation...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb78b34ae5344830b64d938b7997cc86",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/200 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generation complete. Calculating scores...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5e3619b2d934a2894f75f315c23e36f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "092565100dfa4feaa891090b822a23bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d20389bd0e34288afb365e1c0b0b51c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules:   0%|          | 0.00/1.55k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ebbc3a8d9a924e4193feb411927cd97a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading extra modules: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ðŸ“ˆ ROUGE Results ---\n",
      "{'rouge1': 0.29556364855290834, 'rouge2': 0.08060470793226145, 'rougeL': 0.19320904225952973, 'rougeLsum': 0.2576366630349671}\n",
      "\n",
      "--- ðŸ“ˆ BLEU Results ---\n",
      "{'bleu': 0.048683749850367025, 'precisions': [0.40626372936672317, 0.11148541282654294, 0.03875619648490311, 0.015913389421509164], 'brevity_penalty': 0.6696556509349939, 'length_ratio': 0.713780127228743, 'translation_length': 15933, 'reference_length': 22322}\n"
     ]
    }
   ],
   "source": [
    "import evaluate\n",
    "import torch\n",
    "import ast\n",
    "from transformers import GPT2LMHeadModel, GPT2Tokenizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tqdm.auto import tqdm  # For a nice progress bar\n",
    "\n",
    "# --- 1. Load Model (if not already loaded) ---\n",
    "# This assumes 'model' and 'tokenizer' are not in memory\n",
    "# If they are, you can comment out this block.\n",
    "print(\"Loading model and tokenizer...\")\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "model_path = './final_recipe_model'\n",
    "\n",
    "model = GPT2LMHeadModel.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "tokenizer = GPT2Tokenizer.from_pretrained(model_path)\n",
    "print(f\"Model loaded and on device: {device}\")\n",
    "\n",
    "# --- 2. Get our Validation Set ---\n",
    "# We re-create the validation split from our processed dataframe\n",
    "# to get the original, separated columns.\n",
    "try:\n",
    "    _ , val_df = train_test_split(df_processed, test_size=0.1, random_state=42)\n",
    "except NameError:\n",
    "    print(\"Error: 'df_processed' not found. Please re-run the data preparation cells.\")\n",
    "\n",
    "# We'll test on a smaller sample to save time\n",
    "# 200 is a good number for a solid score.\n",
    "EVAL_SAMPLES = 200\n",
    "eval_subset = val_df.sample(n=min(EVAL_SAMPLES, len(val_df)), random_state=42)\n",
    "\n",
    "predictions = []\n",
    "references = []\n",
    "\n",
    "print(f\"Generating {len(eval_subset)} predictions for evaluation...\")\n",
    "\n",
    "# --- 3. Generation Loop ---\n",
    "for _, row in tqdm(eval_subset.iterrows(), total=len(eval_subset)):\n",
    "    \n",
    "    # A. Format the Prompt\n",
    "    title = row['title']\n",
    "    ingredients_list = ast.literal_eval(row['NER'])\n",
    "    ingredients_str = \", \".join(ingredients_list)\n",
    "    prompt_text = f\"TITLE: {title}\\nINGREDIENTS: {ingredients_str}\\nRECIPE:\\n\"\n",
    "    \n",
    "    # B. Generate the Prediction\n",
    "    input_ids = tokenizer.encode(prompt_text, return_tensors='pt').to(device)\n",
    "    prompt_length = input_ids.shape[1]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # We use beam search for evaluation as it's deterministic and high-quality\n",
    "        output_sequences = model.generate(\n",
    "            input_ids=input_ids,\n",
    "            max_length=prompt_length + 250, # Give it 250 tokens for the recipe\n",
    "            num_beams=5,\n",
    "            no_repeat_ngram_size=2,\n",
    "            early_stopping=True,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    # Decode only the newly generated text\n",
    "    pred_text = tokenizer.decode(output_sequences[0][prompt_length:], skip_special_tokens=True)\n",
    "    predictions.append(pred_text)\n",
    "    \n",
    "    # C. Format the Reference (Ground Truth)\n",
    "    ref_list = ast.literal_eval(row['directions'])\n",
    "    ref_text = \"\\n\".join(ref_list)\n",
    "    references.append(ref_text)\n",
    "\n",
    "print(\"Generation complete. Calculating scores...\")\n",
    "\n",
    "# --- 4. Compute Metrics ---\n",
    "rouge = evaluate.load('rouge')\n",
    "bleu = evaluate.load('bleu')\n",
    "\n",
    "rouge_results = rouge.compute(predictions=predictions, references=references)\n",
    "bleu_results = bleu.compute(predictions=predictions, references=references)\n",
    "\n",
    "print(\"\\n--- ðŸ“ˆ ROUGE Results ---\")\n",
    "print(rouge_results)\n",
    "\n",
    "print(\"\\n--- ðŸ“ˆ BLEU Results ---\")\n",
    "print(bleu_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 2957522,
     "sourceId": 5093016,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "project",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
